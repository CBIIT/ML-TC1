{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancer Type Classification using Deep-Learning\n",
    "## S.Ravichandran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document will explain how to use genomic expression data for classifying different cancer/tumor sites/types. This workshop is a follow-up to the NCI-DOE Pilot1 benchmark also called TC1. You can read about the project here, https://github.com/ECP-CANDLE/Benchmarks/tree/master/Pilot1/TC1\n",
    "\n",
    "For classification, we use a Deep-Learning procedure called 1D-Convolutional Neural Network (CONV1D; https://en.wikipedia.org/wiki/Convolutional_neural_network. \n",
    "NCI Genomic Data Commons (GDC; https://gdc.cancer.gov/) is the source of RNASeq expression data. \n",
    "\n",
    "First we will start with genomic data preparation and then we will show how to use the data to build CONV1D model that can classify different cancer types. Please note that there are more than ways to extract data from GDC. What I am describing is one possible way. \n",
    "\n",
    "This is a continuation of data preparation which can be accessed from here, \n",
    "https://github.com/ravichas/ML-TC1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys, gzip, glob, json, time, argparse\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Conv1D, MaxPooling1D, Flatten\n",
    "from keras import optimizers\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.models import Sequential, Model, model_from_json, model_from_yaml\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read features and output files\n",
    "TC1data3 = pd.read_csv(\"Data/TC1-data3stypes.tsv\", sep=\"\\t\", low_memory = False)\n",
    "outcome = pd.read_csv(\"Data/TC1-outcome-data3stypes.tsv\", sep=\"\\t\", low_memory=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome[0].value_counts()\n",
    "outcome = outcome[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(data):\n",
    "    print('Shape of data (BEFORE encode): %s' % str(data.shape))\n",
    "    encoded = to_categorical(data)\n",
    "    print('Shape of data (AFTER  encode): %s\\n' % str(encoded.shape))\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data (BEFORE encode): (150,)\n",
      "Shape of data (AFTER  encode): (150, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "outcome = encode(outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(TC1data3, outcome,\n",
    "                                                    train_size=0.75,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=123,\n",
    "                                                    stratify = outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "conv=[128, 20, 1, 128, 10, 1]\n",
    "dense=[200,20]\n",
    "activation='relu'\n",
    "batch_size=20\n",
    "\n",
    "# Number of sites\n",
    "classes=3\n",
    "conv=[128, 20, 1, 128, 10, 1]\n",
    "dense = [200,20]\n",
    "drop = 0.1\n",
    "feature_subsample = 0\n",
    "loss='categorical_crossentropy'\n",
    "# metrics='accuracy'\n",
    "out_act='softmax'\n",
    "pool=[1, 10]\n",
    "# optimizer='sgd'\n",
    "shuffle = False\n",
    "epochs=400\n",
    "\n",
    "optimizer = optimizers.SGD(lr=0.1)\n",
    "metrics = ['acc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_len = X_train.shape[1]\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = 128\n",
    "filter_len = 20\n",
    "stride = 1\n",
    "\n",
    "pool_list = [1,10]\n",
    "dense_first = False\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "dense_first = True\n",
    "\n",
    "# model.add  CONV1D\n",
    "model.add(Conv1D(filters = filters,\n",
    "                 kernel_size = filter_len,\n",
    "                 strides = stride,\n",
    "                 padding='valid',\n",
    "                 input_shape=(x_train_len, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 60464, 128)        2688      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 60464, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 60464, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 60455, 128)        163968    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 60455, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 6045, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 773760)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               154752200 \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                4020      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 63        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 154,922,939\n",
      "Trainable params: 154,922,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Activation\n",
    "model.add(Activation('relu'))\n",
    "# MaxPooling\n",
    "model.add(MaxPooling1D(pool_size = 1))\n",
    "\n",
    "filters = 128\n",
    "filter_len = 10\n",
    "stride = 1\n",
    "\n",
    "# Conv1D\n",
    "model.add(Conv1D(filters=filters,\n",
    "                 kernel_size=filter_len,\n",
    "                 strides=stride,\n",
    "                 padding='valid'))\n",
    "# Activation\n",
    "model.add(Activation('relu'))\n",
    "# MaxPooling\n",
    "model.add(MaxPooling1D(pool_size = 10))\n",
    "\n",
    "# Flatten\n",
    "model.add(Flatten())\n",
    "# Dense\n",
    "model.add(Dense(200))\n",
    "# activation\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#dropout\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "#Dense\n",
    "model.add(Dense(20))\n",
    "#Activation\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#dropout\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation(out_act))\n",
    "\n",
    "model.compile( loss= loss,\n",
    "              optimizer = optimizer,\n",
    "              metrics = metrics )\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 112 samples, validate on 38 samples\n",
      "Epoch 1/400\n",
      "112/112 [==============================] - 192s 2s/step - loss: 1.6281 - acc: 0.2679 - val_loss: 1.1053 - val_acc: 0.3421\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.10528, saving model to ./tc1.autosave.model.h5\n",
      "Epoch 2/400\n",
      "112/112 [==============================] - 204s 2s/step - loss: 1.1123 - acc: 0.3393 - val_loss: 1.0975 - val_acc: 0.3158\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.10528 to 1.09748, saving model to ./tc1.autosave.model.h5\n",
      "Epoch 3/400\n",
      "112/112 [==============================] - 194s 2s/step - loss: 1.0919 - acc: 0.4018 - val_loss: 1.0758 - val_acc: 0.6579\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.09748 to 1.07583, saving model to ./tc1.autosave.model.h5\n",
      "Epoch 4/400\n",
      "112/112 [==============================] - 184s 2s/step - loss: 1.0564 - acc: 0.4821 - val_loss: 1.0592 - val_acc: 0.3421\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.07583 to 1.05917, saving model to ./tc1.autosave.model.h5\n",
      "Epoch 5/400\n",
      "112/112 [==============================] - 182s 2s/step - loss: 1.0799 - acc: 0.4107 - val_loss: 0.9771 - val_acc: 0.5789\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.05917 to 0.97711, saving model to ./tc1.autosave.model.h5\n",
      "Epoch 6/400\n",
      "112/112 [==============================] - 182s 2s/step - loss: 1.0058 - acc: 0.6518 - val_loss: 0.9310 - val_acc: 0.3684\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.97711 to 0.93097, saving model to ./tc1.autosave.model.h5\n",
      "Epoch 7/400\n",
      "112/112 [==============================] - 197s 2s/step - loss: 0.9626 - acc: 0.6161 - val_loss: 1.2176 - val_acc: 0.6579\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.93097\n",
      "Epoch 8/400\n",
      " 40/112 [=========>....................] - ETA: 1:52 - loss: 1.0165 - acc: 0.4500"
     ]
    }
   ],
   "source": [
    "# save\n",
    "save = '.'\n",
    "output_dir = \"Output\"\n",
    "\n",
    "output_dir = save\n",
    "if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "model_name = 'tc1'\n",
    "path = '{}/{}.autosave.model.h5'.format(output_dir, model_name)\n",
    "checkpointer = ModelCheckpoint(filepath=path,\n",
    "                               verbose=1,\n",
    "                               save_weights_only=False,\n",
    "                               save_best_only=True)\n",
    "\n",
    "csv_logger = CSVLogger('{}/training.log'.format(output_dir))\n",
    "\n",
    "# SR: change epsilon to min_delta\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.1,\n",
    "                              patience=10,\n",
    "                              verbose=1, mode='auto',\n",
    "                              min_delta=0.0001,\n",
    "                              cooldown=0,\n",
    "                              min_lr=0)\n",
    "# batch_size = 20\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size,\n",
    "                    epochs=epochs, verbose=1, validation_data=(X_test, Y_test),\n",
    "                    callbacks = [checkpointer, csv_logger, reduce_lr])\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"{}/{}.model.h5\".format(output_dir, model_name))\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model_yaml.load_weights('{}/{}.model.h5'.format(output_dir, model_name))\n",
    "print(\"Loaded yaml model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
